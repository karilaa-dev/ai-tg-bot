---
last_mapped: 2026-01-27T00:00:00Z
total_files: 27
total_tokens: 10364
---

# Codebase Map

> Auto-generated by Cartographer. Last mapped: 2026-01-27

## System Overview

```mermaid
graph TB
    subgraph Entry
        Main[main.py]
    end

    subgraph Polling
        Poll[polling.py]
    end

    subgraph Handlers
        Cmd[commands.py]
        Msg[message.py]
        TG[telegram.py]
    end

    subgraph AI
        OR[openrouter.py]
        Tools[tools.py]
    end

    subgraph Database
        Repo[repository.py]
        Models[models.py]
    end

    subgraph External Tools
        Tavily[tavily.py]
    end

    subgraph Utils
        Format[formatting.py]
        Tokens[tokens.py]
        Image[image.py]
        PDF[pdf.py]
    end

    Main --> Poll
    Main --> Repo
    Poll --> Cmd
    Poll --> Msg
    Poll --> TG
    Cmd --> Repo
    Cmd --> TG
    Msg --> OR
    Msg --> Repo
    Msg --> TG
    Msg --> Format
    Msg --> Tokens
    Msg --> Image
    Msg --> PDF
    OR --> Tools
    OR --> Tavily
    Repo --> Models
```

## Directory Structure

```
ai-tg-bot/
├── main.py              # Application entry point
├── pyproject.toml       # Project dependencies and metadata
├── .env.example         # Environment variables template
├── bot/
│   ├── __init__.py
│   ├── config.py        # Pydantic settings (singleton)
│   ├── polling.py       # Telegram long polling loop
│   ├── ai/
│   │   ├── __init__.py
│   │   ├── openrouter.py  # AI client with streaming & tool calling
│   │   └── tools.py       # OpenAI function calling schemas
│   ├── database/
│   │   ├── __init__.py
│   │   ├── models.py      # SQLAlchemy ORM models
│   │   └── repository.py  # Data access layer
│   ├── handlers/
│   │   ├── __init__.py
│   │   ├── commands.py    # /thinking command handler
│   │   ├── message.py     # Core message processing
│   │   └── telegram.py    # Telegram Bot API client
│   ├── tools/
│   │   ├── __init__.py
│   │   └── tavily.py      # Web search & extraction
│   └── utils/
│       ├── __init__.py
│       ├── formatting.py  # Telegram MarkdownV2 formatting
│       ├── image.py       # Image download & base64 encoding
│       ├── pdf.py         # PDF download & base64 encoding
│       └── tokens.py      # Token counting & context trimming
└── docs/
    └── CODEBASE_MAP.md    # This file
```

## Module Guide

### Entry Point (`main.py`)

**Purpose**: Application lifecycle management
**Key functions**: `main()` - initializes database, starts polling, handles shutdown

### Configuration (`bot/config.py`)

**Purpose**: Centralized environment-based configuration
**Exports**: `Settings`, `settings` (singleton)
**Pattern**: Pydantic settings with `.env` file support

**Environment Variables**:
| Variable | Required | Default | Description |
|----------|----------|---------|-------------|
| `TELEGRAM_BOT_TOKEN` | Yes | - | Bot authentication |
| `OPENROUTER_API_KEY` | Yes | - | AI API access |
| `OPENROUTER_MODEL` | No | `moonshotai/kimi-k2.5` | Model ID |
| `TAVILY_API_KEY` | Yes | - | Web search API |
| `DATABASE_URL` | No | `sqlite+aiosqlite:///./bot.db` | Database connection |
| `CONTEXT_TOKEN_LIMIT` | No | `8000` | Max context tokens |

### AI Module (`bot/ai/`)

#### `openrouter.py`
**Purpose**: OpenRouter API client with streaming and tool calling
**Exports**: `StreamChunk`, `OpenRouterClient`, `openrouter_client` (singleton)
**Key method**: `generate_response_stream()` - async generator yielding content/reasoning chunks
**Dependencies**: OpenAI SDK, Tavily client

**Gotchas**:
- Max 6 tool calling iterations (hardcoded)
- Reasoning extracted from `delta.model_extra.get("reasoning")` (model-specific)
- Only supports `web_search` and `extract_webpage` tools

#### `tools.py`
**Purpose**: OpenAI function calling schema definitions
**Exports**: `TOOLS` - list of tool schemas

### Database Module (`bot/database/`)

#### `models.py`
**Purpose**: SQLAlchemy 2.0 ORM models
**Exports**: `Base`, `User`, `Conversation`, `Message`

**Schema**:
```
User (telegram_id, username, full_name, show_thinking)
  └── Conversation (user_id, chat_id, thread_id)
        └── Message (role, content, image_file_id, pdf_file_id, created_at)
```

**Gotchas**:
- Uses `BigInteger` for Telegram IDs
- `show_thinking` is per-user, not per-conversation
- Uses deprecated `datetime.utcnow` (Python 3.12+)

#### `repository.py`
**Purpose**: Data access layer with CRUD operations
**Exports**: `Repository`, `repository` (singleton)
**Pattern**: Repository pattern with async SQLAlchemy sessions

**Key methods**:
- `get_or_create_user()` / `get_or_create_conversation()`
- `add_message()` / `get_conversation_with_messages()`
- `trim_messages_to_limit()` - removes old messages beyond token limit

### Handlers Module (`bot/handlers/`)

#### `telegram.py`
**Purpose**: Low-level Telegram Bot API HTTP client
**Exports**: `TelegramClient`, `telegram_client` (singleton)
**Uses**: aiohttp for async HTTP requests

**Key methods**:
- `send_message()` / `send_message_draft()` (Bot API 9.3)
- `finalize_draft()` - converts draft to permanent message
- `get_file()` / `download_file()` - file handling

#### `commands.py`
**Purpose**: Command handlers
**Exports**: `handle_thinking_command()` - toggles thinking trace visibility

#### `message.py`
**Purpose**: Core message processing with streaming responses
**Exports**: `handle_message()`, `build_content()`, `format_history()`

**Data flow**:
1. Load user, conversation, message history
2. Build multimodal content (text + images + PDFs)
3. Trim to token limit
4. Stream AI response with draft updates every 0.5s
5. Handle message splitting for long responses
6. Save assistant response to database

**Gotchas**:
- Separate draft IDs for thinking vs content
- Code block state tracked across message splits
- Falls back to plain text on MarkdownV2 parse errors
- Rate limiting handled with retry_after

### Tools Module (`bot/tools/`)

#### `tavily.py`
**Purpose**: Async wrapper for Tavily web search/extraction
**Exports**: `TavilyClient`, `tavily_client` (singleton)

**Methods**:
- `search(query)` - web search (max 5 results, basic depth)
- `extract(url)` - webpage content extraction

**Gotchas**:
- Returns error dict instead of raising exceptions
- Only uses first result from extract response

### Utils Module (`bot/utils/`)

#### `formatting.py`
**Purpose**: Telegram MarkdownV2 text formatting
**Exports**: `SAFE_MESSAGE_LENGTH` (3900), escaping functions, `format_thinking_block()`, `split_message()`

**Gotchas**:
- Safe length is 3900 (Telegram limit is 4096)
- Expandable blockquote format: `**>` first line, `||` last line
- Split prefers paragraph > line > sentence > word boundaries

#### `tokens.py`
**Purpose**: Token counting and context window management
**Exports**: `get_encoding()`, `count_tokens()`, `count_message_tokens()`, `trim_messages_to_limit()`

**Gotchas**:
- Uses `cl100k_base` encoding (OpenAI standard)
- Images/files estimated at 85 tokens each
- 4 base tokens added per message for formatting overhead

#### `image.py` / `pdf.py`
**Purpose**: Download and base64 encode files from Telegram
**Pattern**: Returns data URL string (`data:mime/type;base64,...`)
**Gotchas**: Returns `None` on error (silent failure with logging)

## Data Flow

### Message Processing Flow

```mermaid
sequenceDiagram
    participant TG as Telegram
    participant Poll as polling.py
    participant Msg as message.py
    participant DB as repository.py
    participant AI as openrouter.py
    participant Tool as tavily.py

    TG->>Poll: Update (message)
    Poll->>Msg: handle_message()
    Msg->>DB: get_or_create_user/conversation
    Msg->>DB: add_message (user)
    Msg->>DB: get_conversation_with_messages
    Msg->>Msg: build_content (images/PDFs)
    Msg->>Msg: trim_messages_to_limit

    loop Streaming
        Msg->>AI: generate_response_stream()
        AI->>AI: Call OpenRouter API

        opt Tool Calling
            AI->>Tool: web_search / extract_webpage
            Tool-->>AI: Results
        end

        AI-->>Msg: StreamChunk
        Msg->>TG: send_message_draft (every 0.5s)
    end

    Msg->>TG: finalize_draft
    Msg->>DB: add_message (assistant)
```

### /thinking Command Flow

```mermaid
sequenceDiagram
    participant TG as Telegram
    participant Poll as polling.py
    participant Cmd as commands.py
    participant DB as repository.py

    TG->>Poll: /thinking command
    Poll->>Cmd: handle_thinking_command()
    Cmd->>TG: send_message_draft
    Cmd->>DB: toggle_thinking()
    Cmd->>TG: finalize_draft (new status)
```

## Conventions

### Code Style
- Python 3.12+ with type hints
- Async-first design (all I/O is async)
- SQLAlchemy 2.0 declarative style with `Mapped` annotations
- Pydantic for settings validation

### Patterns
- **Singleton**: All clients use global singleton instances
- **Repository**: Database access abstracted through Repository class
- **Async Generator**: Streaming responses via `async for`
- **Draft Messages**: Real-time updates using Telegram Bot API 9.3

### Naming
- `*_client` suffix for API clients
- `handle_*` prefix for handlers
- `get_or_create_*` for upsert operations
- `*_file_id` for Telegram file references

## Gotchas

### Critical
1. **Unhandled task exceptions**: Background tasks in `polling.py` created without awaiting
2. **Silent failures**: Utils return `None` on error instead of raising
3. **Deprecated datetime**: `datetime.utcnow` used in models (Python 3.12+ deprecation)

### Important
4. **Model-specific behavior**: Reasoning extracted from `model_extra.get("reasoning")`
5. **Hardcoded limits**: Max 6 tool rounds, 3900 char message length, 85 tokens per image
6. **Rate limiting**: Handled for finalized messages, skipped for drafts

### Architectural
7. **Fire-and-forget**: Updates processed as background tasks (potential lost errors)
8. **Manual transactions**: Repository uses `flush()`, caller must `commit()`
9. **Thread safety**: Global singletons may cause issues in multi-threaded environments

## Navigation Guide

**To add a new command**:
1. Add handler function in `bot/handlers/commands.py`
2. Add routing in `bot/polling.py:process_update()`

**To add a new AI tool**:
1. Add schema in `bot/ai/tools.py`
2. Add execution logic in `bot/ai/openrouter.py:generate_response_stream()` (tool_call handling)
3. Add client in `bot/tools/` if external API needed

**To modify database schema**:
1. Update models in `bot/database/models.py`
2. Update repository methods in `bot/database/repository.py`
3. Handle migration (manual - no Alembic configured)

**To change AI model**:
1. Update `OPENROUTER_MODEL` in `.env`
2. Check if model supports reasoning field in `model_extra` (for thinking traces)

**To add new message content type**:
1. Add utility in `bot/utils/` (like `image.py`, `pdf.py`)
2. Update `bot/handlers/message.py:build_content()` to handle new type
3. Optionally add field to Message model for file_id storage
